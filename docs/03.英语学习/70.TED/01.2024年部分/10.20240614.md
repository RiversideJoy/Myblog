---
title: 20240614
date: 2024-06-14 20:10:59
permalink: /pages/b302f4/
author:
  name: Riverside Joy
  link: https://github.com/MaiRen1997
categories:
  - 英语学习
  - TED
  - 2024年部分
tags:
  - 
---
Now, I haven't met most of you or really any of you,
but I feel a really good vibe in the room.

(Laughter)

And so I think I'd like to treat you all to a meal.

What do you think?

Yes? Great, so many new friends.

So we're going to go to this cafe,
they serve sandwiches.

And the sandwiches are really delicious.

But I have to tell you that sometimes they make people really, really sick.

(Laughter)
And we don't know why.

Because the cafe won't tell us how they make the sandwich,
they won't tell us about the ingredients.

And then the authorities have no way to fix the problem.

But the offer still stands.

So who wants to get a sandwich?

(Laughter)
Some brave souls, we can talk after.

But for the rest of you, I understand.

You don't have enough information
to make good choices about your safety
or even fix the issue.

Now, before I further the anxiety here, I'm not actually trying to make you sick,
but this is an analogy to how we're currently making algorithmic systems,
also known as artificial intelligence or AI.

Now, for those who haven't thought about the relationship
between AI and sandwiches, don't worry about it,
I'm here for you, I'm going to explain.

You see, AI systems, they provide benefit to society.

They feed us,
but they're also inconsistently making us sick.

And we don't have access to the ingredients that go into the AI.
And so we can't actually address the issues.
We also can't stop eating AI
like we can just stop eating a shady sandwich
because it's everywhere,
and we often don't even know that we're encountering a system
that's algorithmically based.
So today, I'm going to tell you about some of the AI trends that I see.
I'm going to draw on my experience building these systems
over the last two decades to tell you about the tools
that I and others have built to look into these AI ingredients.
And finally, I'm going to leave you with three principles
that I think will give us a healthier relationship
to the companies that build artificial intelligence.
I'm going to start with the question, how did we get here?
AI is not new.
We have been living alongside AI for two decades.
Every time that you apply for something online,
you open a bank account or you go through passport control,
you're encountering an algorithmic system.
We've also been living with the negative repercussions of AI for 20 years,
and this is how it makes us sick.
These systems get deployed on broad populations,
and then certain subsets end up getting negatively disparately impacted,
usually on the basis of race or gender or other characteristics.
We need to be able to understand the ingredients to these systems
so that we can address the issues.
So what are the ingredients to an AI system?
Well, data fuels the AI.
The AI is going to look like the data that you gave it.
So for example,
if I want to make a risk-assessment system for diabetes,
my training data set might be adults in a certain region.
And so I'll build that system,
it'll work really well for those adults in that region.
But it does not work for adults in other regions
or maybe at all for children.
So you can imagine if we deploy this for all those populations,
there are going to be a lot of people who are harmed.
We need to be able to understand the quality of the data before we use it.
But I'm sorry to tell you that we currently live
in what I call the Wild West of data.
It's really hard to assess quality of data before you use it.
There are no global standards for data quality assessment,
and there are very few data regulations around how you can use data
and what types of data you can use.
This is kind of like in the food safety realm.
If we couldn't understand where the ingredients were sourced,
we also had no idea whether they were safe for us to consume.
We also tend to stitch data together,
and every time we stitch this data together,
which we might find on the internet, scrape, we might generate it,
we could source it.
We lose information about the quality of the data.
And the folks who are building the models
are not the ones that found the data.
So there's further information that's lost.
Now, I've been asking myself a lot of questions
about how can we understand the data quality before we use it.
And this emerges from two decades of building these kinds of systems.
The way I was trained to build systems is similar to how people do it today.
You build for the middle of the distribution.
That's your normal user.
So for me, a lot of my training data sets
would include information about people from the Western world who speak English,
who have certain normative characteristics.
And it took me an embarrassingly long amount of time
to realize that I was not my own user.
So I identify as non-binary, as mixed race,
I wear a hearing aid
and I just wasn't represented in the data sets that I was using.
And so I was building systems that literally didn't work for me.
And for example, I once built a system that repeatedly told me
that I was a white Eastern-European lady.
This did a real number on my identity.
(Laughter)
But perhaps even more worrying,
this was a system to be deployed in health care,
where your background can determine things like risk scores for diseases.
And so I started to wonder,
can I build tools and work with others to do this
so that I can look inside of a dataset before I use it?
In 2018, I was part of a fellowship at Harvard and MIT,
and I, with some colleagues, decided to try to address this problem.
And so we launched the Data Nutrition Project,
which is a research group and also a nonprofit
that builds nutrition labels for datasets.
So similar to food nutrition labels,
the idea here is that you can look inside of a data set before you use it.
You can understand the ingredients,
see whether it's healthy for the things that you want to do.
Now this is a cartoonified version of the label.
The top part tells you about the completion of the label itself.
And underneath that you have information about the data,
the description, the keywords, the tags,
and importantly, on the right hand side,
how you should and should not use the data.
If you could scroll on this cartoon,
you would see information about risks and mitigation strategies
across a number of vectors.
And we launched this with two audiences in mind.
The first audience are folks who are building AI.
So they’re choosing datasets.
We want to help them make a better choice.
The second audience are folks who are building datasets.
And it turns out
that when you tell someone they have to put a label on something,
they think about the ingredients beforehand.
The analogy here might be,
if I want to make a sandwich and say that it’s gluten-free,
I have to think about all the components as I make the sandwich,
the bread and the ingredients, the sauces.
I can't just put it on a sandwich and put it in front of you
and tell you it's gluten-free.
We're really proud of the work that we've done.
We launched this as a design and then a prototype
and ultimately a tool for others to make their own labels.
And we've worked with experts at places like Microsoft Research,
the United Nations and professors globally
to integrate the label and the methodology
into their work flows and into their curricula.
But we know it only goes so far.
And that's because it's actually really hard to get a label
on every single dataset.
And this comes down to the question
of why would you put a label on a dataset to begin with?
Well, the first reason is not rocket science.
It's that you have to.
And this is, quite frankly, why food nutrition labels exist.
It's because if they didn't put them on the boxes, it would be illegal.
However, we don't really have AI regulation.
We don't have much regulation around the use of data.
Now there is some on the horizon.
For example, the EU AI Act just passed this week.
And although there are no requirements around making the training data available,
they do have provisions for creating transparency labeling
like the dataset nutrition label, data sheets, data statements.
There are many in the space.
We think this is a really good first step.
The second reason that you might have a label on a dataset
is because it is a best practice or a cultural norm.
The example here might be how we're starting to see
more and more food packaging and menus at restaurants
include information about whether there's gluten.
This is not required by law,
although if you do say it, it had better be true.
And the reason that people are adding this to their menus
and their food packaging
is because there's an increased awareness of the sensitivity
and kind of the seriousness of that kind of an allergy or condition.
So we're also seeing some movement in this area.
Folks who are building datasets are starting to put nutrition labels,
data sheets on their datasets.
And people who are using data are starting to request the information.
This is really heartening.
And you might say, "Kasia, why are you up here?
Everything seems to be going well, seems to be getting better."
In some ways it is.
But I'm also here to tell you that our relationship to data
is getting worse.
Now the last few years have seen a supercharged interest
in gathering datasets.
Companies are scraping the web.
They're transcribing millions of hours of YouTube videos into text.
By some estimates, they'll run out of information on the internet by 2026.
They're even considering buying publishing houses
so they can get access to printed text and books.
So why are they gathering this information?
Well, they need more and more information
to train a new technique called generative AI.
I want to tell you about the size of these datasets.
If you look at GPT-3, which is a model that launched in 2020,
the training dataset included 300 billion words, or parts of words.
Now for context, the English language contains less than a million words.
Just three years later, DBRX was launched,
which was trained on eight trillion words.
So 300 billion to eight trillion in three years.
And the datasets are getting bigger.
Now with each successive model launch,
the datasets are actually less and less transparent.
And even we have access to the information,
it's so big, it's so hard to look inside without any kind of transparency tooling.
And the generative AI itself is also causing some worries.
And you've probably encountered this technique through ChatGPT.
I don't need to know what you do on the internet,
that's between you and the internet,
but you probably know, just like I do,
how easy it is to create information using ChatGPT
and other generative AI technologies
and to put that out onto the web.
And so we're looking at a situation
in which we're going to encounter lots of information
that's algorithmically generated but we won't know it
and we won't know whether it's true.
And this increases the scale of the potential risks and harms from AI.
Not only that, I'm sorry,
but the models themselves are getting controlled
by a smaller and smaller number of private actors in US tech firms.
So this is the models that were launched last year, in 2023.
And you can see most of them are pink, meaning they came out of industry.
And if you look at this over time, more and more are coming out of industry
and fewer and fewer are coming out of all the other sectors combined,
including academia and government,
where technology is often launched in a way
that's more easy to be scrutinized.
So if we go back to our cafe analogy,
this is like you have a small number of private actors
who own all the ingredients,
they make all the sandwiches globally,
and there's not a lot of regulation.
And so at this point you're probably scared
and maybe feeling a little uncomfortable.
Which is ironic because a few minutes ago, I was going to get you all sandwiches
and you said yes.
This is why you should not accept food from strangers.
But I wouldn't be up here if I weren't also optimistic.
And that's because I think we have momentum
behind the regulation and the culture changes.
Especially if we align ourselves with three basic principles
about how corporations should engage with data.
The first principle is that companies that gather data should tell us
what they're gathering.
This would allow us to ask questions like, is it copyrighted material?
Is that information private?
Could you please stop?
It also opens up the data to scientific inquiry.
The second principle is that companies that are gathering our data should tell us
what they're going to do with it before they do anything with it.
And by requiring that companies tell us their plan,
this means that they have to have a plan,
which would be a great first step.
It also probably would lead to the minimization of data capture,
because they wouldn't be able to capture data
if they didn't know what they were already going to do with it.
And finally, principle three,
companies that build AI should tell us about the data
that they use to train the AI.
And this is where dataset nutrition labels
and other transparency labeling comes into play.
You know, in the case where the data itself won't be made available,
which is most of the time, probably,
the labeling is critical for us to be able to investigate the ingredients
and start to find solutions.
So I want to leave you with the good news,
and that is that the data nutrition projects and other projects
are just a small part of a global movement
towards AI accountability.
Dataset Nutrition Label and other projects are just a first step.
Regulation's on the horizon,
the cultural norms are shifting,
especially if we align with these three basic principles
that companies should tell us what they're gathering,
tell us what they're going to do with it before they do anything with it,
and that companies that are building AI
should explain the data that they're using to build the system.
We need to hold these organizations accountable
for the AI that they're building
by asking them, just like we do with the food industry,
what's inside and how did you make it?
Only then can we mitigate the issues before they occur,
as opposed to after they occur.
And in doing so, create an integrated algorithmic internet
that is healthier for everyone.
Thank you.
(Applause)

"现在，我还没有见过你们中的大多数人，
也没有见过你们中的任何一个人，"
但我感觉房间里的气氛非常好。
（笑声）
"所以我想请
大家吃一顿饭。"
你怎么认为？
是的？ 太好了，这么多新朋友。
所以我们要去这家咖啡馆，
他们提供三明治。
而且三明治真的很好吃。
"但我必须告诉你，有时
它们会让人真的非常难受。"
（笑声）
我们不知道为什么。
"因为咖啡馆不会告诉我们
他们如何制作三明治，也"
不会告诉我们成分。
"然后当局就
没有办法解决这个问题。"
但优惠仍然有效。
那么谁想吃三明治呢？
（笑声）
一些勇敢的人，我们可以聊聊。
但对于你们其他人，我理解。
您没有足够的信息
来就您的安全做出正确的选择，
甚至无法解决问题。
"现在，在我进一步加剧焦虑之前，
我实际上并不是想让你生病，"
"但这与我们
目前如何制作算法系统（"
"也称为
人工智能或人工智能）进行了类比。"
"现在，对于那些还没有思考
过"
"人工智能和三明治之间关系的人，
别担心，"
我在这里为你解释。
"你看，人工智能系统，
它们为社会带来好处。"
它们为我们提供食物，
"但也时常
让我们生病。"
"而且我们无法获得
人工智能的成分。"
"所以我们无法真正
解决这些问题。"
我们也无法停止吃人工智能，
"就像我们不能停止
吃可疑的三明治一样，"
因为它无处不在，
"而且我们常常甚至不知道
我们正在遇到一个基于"
算法的系统。
"所以今天，我将向大家介绍
我所看到的一些人工智能趋势。"
"我将
"
"利用过去二十年构建这些系统的经验，
向您介绍"
"我和其他人为
研究这些人工智能成分而构建的工具。"
"最后，我将向大家介绍
三项原则，"
"我认为这将使我们
"
"与
构建人工智能的公司建立更健康的关系。"
"我首先要问一个问题：
我们是如何走到这一步的？"
人工智能并不新鲜。
"我们与
人工智能一起生活了二十年。"
"每次你
在网上申请东西、"
"开立银行账户
或通过护照检查时，"
你都会遇到一个算法系统。
"
20 年来，我们一直承受着人工智能的负面影响，"
这就是它让我们生病的原因。
"这些系统被部署
在广泛的人群中，"
"然后某些子集最终会受到不同的
负面影响，"
"通常是基于种族
、性别或其他特征。"
"我们需要能够了解
这些系统的组成部分，"
以便我们能够解决问题。
"那么人工智能系统的组成部分是什么
？"
好吧，数据为人工智能提供动力。
"人工智能将看起来
像你提供给它的数据。"
例如，
"如果我想做一个糖尿病风险评估
系统，"
"我的训练数据集可能是
某个地区的成年人。"
所以我将建立这个系统，
"它对于
该地区的成年人来说非常有效。"
"但它对
其他地区的成年人不起作用，"
甚至对儿童根本不起作用。
"所以你可以想象，如果我们
为所有这些人群部署这个，"
"将会有
很多人受到伤害。 在使用数据之前，"
"我们需要能够了解
数据的质量。"
"但我很遗憾地告诉您，
我们目前生活"
在我所说的数据狂野西部。
"
在使用数据之前评估数据的质量确实很困难。"
"
数据质量评估没有全球标准，"
"
关于如何使用数据"
以及可以使用什么类型的数据法规也很少。
"这有点像
食品安全领域。"
"如果我们无法了解
这些成分的来源，"
"我们也不知道
它们是否可以安全食用。"
我们还倾向于将数据拼接在一起，
"每次我们将
这些数据拼接在一起时，"
"我们可能会在互联网上找到这些数据，
刮擦，我们可能会生成它，"
我们可以获取它。
"我们丢失了
有关数据质量的信息。"
构建模型的人
并不是发现数据的人。
"所以还有更多的
信息丢失了。"
"现在，我一直在问自己
很多问题，在"
"
使用数据之前如何了解数据质量。"
"这是二十年
来构建此类系统的结果。"
"我接受构建系统培训的方式
与今天人们的方式类似。"
"您为
分布的中间构建。"
那是你的普通用户。
因此，对我来说，我的很多训练数据集
"都包含
来自西方世界的说英语的人的信息，"
"这些人具有某些
规范特征。"
"我花了很
长一段时间才"
意识到我不是我自己的用户。
"因此，我认为自己是非二元性别，
是混血儿，"
我戴着助听器，
"只是没有出现
在我使用的数据集中。"
"所以我正在构建的系统
实际上并不适合我。"
"例如，我曾经建立了一个系统，
它反复告诉我，"
我是一位东欧白人女士。
这对我的身份产生了真实的影响。
（笑声）
但也许更令人担忧的是，
"这是一个
部署在医疗保健领域的系统，"
"你的背景可以决定
诸如疾病风险评分之类的事情。"
所以我开始想知道，
"我可以构建工具并与
其他人合作来完成此操作，"
"以便我可以
在使用数据集之前查看它的内部情况吗？"
"2018 年，我成为
哈佛大学和麻省理工学院的研究员，"
"我和一些同事
决定尝试解决这个问题。"
"因此，我们启动了
数据营养项目，"
"这是一个研究小组，
也是一个"
"
为数据集构建营养标签的非营利组织。"
与食品营养标签类似，
"这里的想法是您可以
在使用数据集之前查看其内部。"
您可以了解其成分，
"看看它是否适合
您想做的事情。"
"现在这是
该标签的卡通化版本。"
"顶部部分告诉您
标签本身的完成情况。"
"在其下方，您可以
了解有关数据、"
描述、关键字、标签的信息，
重要的是，在右侧，您可以了解
"应该如何使用
和不应该如何使用数据。"
如果您可以滚动此漫画，
"您会看到有关多个向量的
风险和缓解策略的信息"
。
"我们推出这个产品时
考虑到了两个受众。 第"
"一批受众是
正在构建人工智能的人们。"
所以他们正在选择数据集。
我们希望帮助他们做出更好的选择。
"第二个受众是
正在构建数据集的人。 事实"
证明，
"当你告诉某人
必须在某物上贴上标签时，"
"他们会
事先考虑成分。"
这里的类比可能是，
"如果我想做一个三明治
并说它不含麸质，"
"我必须在
制作三明治时考虑所有成分，"
面包和配料，酱汁。
"我不能只是把它放在三明治上
然后放在你面前"
并告诉你它不含麸质。
"我们
对我们所做的工作感到非常自豪。"
"我们将其作为设计推出，
然后是原型，"
"最终成为其他人
制作自己标签的工具。"
"我们与
微软研究院、联合国等机构的专家"
以及全球教授合作，将
标签和方法整合
"到他们的工作流程
和课程中。"
但我们知道这只是到目前为止。
"那是因为实际上
很难"
在每个数据集上获得标签。
这归结为一个问题：
"为什么要
在数据集上添加标签？"
"嗯，第一个原因
不是火箭科学。"
这是你必须这样做的。
"坦率地说，这就是
食品营养标签存在的原因。"
"因为如果他们不把它们贴
在盒子上，那就是非法的。"
"然而，我们并没有真正的
人工智能监管。"
"我们
对数据的使用没有太多监管。"
现在有一些即将出现。
"例如，欧盟人工智能法案
本周刚刚通过。"
"尽管没有
关于提供训练数据的要求，但"
"他们确实有创建
透明度标签的规定，"
"例如数据集营养标签、
数据表、数据声明。"
空间里有很多。
我们认为这是非常好的第一步。 数据集上
"可能有标签的第二个原因
"
"是它是最佳实践
或文化规范。"
"这里的例子可能是
我们如何开始看到"
"越来越多的
餐馆的食品包装和菜单"
"包含
有关是否含有麸质的信息。"
法律没有要求这样做，
"但如果你确实这么说了，
最好是真的。"
"人们之所以将
其添加到菜单"
和食品包装中，
"是因为人们越来越
意识到"
"
这种过敏或病症的敏感性和严重性。"
"所以我们也看到
这个领域发生了一些变化。"
"构建数据集的人们
开始"
在他们的数据集上添加营养标签、数据表。
"使用数据的人
开始请求信息。"
这真是令人振奋。
"你可能会说，“卡西亚，
你为什么在这里？"
"一切似乎都很顺利，
似乎正在变得更好。”"
在某些方面确实如此。
"但我也想告诉你们，
我们与数据的关系"
正在变得越来越糟。
"现在，过去几年人们
"
对收集数据集的兴趣越来越浓厚。
公司正在抓取网络。
"他们正在将数百万小时
的 YouTube 视频转录成文本。"
"据估计，
到 2026 年，他们将耗尽互联网上的信息。"
"他们甚至考虑购买
出版社，"
"以便获得
印刷文本和书籍。"
"那么他们为什么要收集
这些信息呢？"
"嗯，他们需要
越来越多的信息"
"来训练一种
称为生成人工智能的新技术。"
"我想告诉您
这些数据集的大小。"
"如果你看一下
2020 年推出的模型 GPT-3，"
"训练数据集包括
3000 亿个单词或部分单词。"
"现在就上下文而言，英语
包含的单词不到一百万个。"
仅仅三年后，DBRX 就推出了，
它接受了 8 万亿字的训练。
"所以三年内3000亿到8
万亿。"
而且数据集越来越大。
现在，随着模型的不断推出，
"数据集实际上
越来越不透明。"
"即使我们可以
访问这些信息，但"
"它太大了，如果
没有任何透明工具，就很难了解其内部情况。"
"而生成式人工智能本身
也引起了一些担忧。"
"您可能已经
通过 ChatGPT 遇到过这种技术。"
"我不需要知道
你在互联网上做什么，"
那是你和互联网之间的事情，
但你可能知道，就像我一样，
"
使用 ChatGPT"
和其他生成式人工智能技术创建信息
并将其发布出来是多么容易 到网上。
因此，我们正在考虑这样一种情况，
"我们将遇到
大量通过"
"算法生成的信息，
但我们不知道它"
，也不知道它是否真实。
"这增加了
人工智能潜在风险和危害的规模。"
抱歉，不仅如此，
"模型本身也
被"
"
美国科技公司中越来越少的私人参与者所控制。"
"这是
去年 2023 年推出的型号。"
"你可以看到它们大部分是粉红色的，
这意味着它们是工业界出来的。"
"如果你随着时间的推移观察这一点，就会发现
越来越多的产品来自工业界，而"
"来自
所有其他部门的人越来越少，"
包括学术界和政府，这些领域的
"技术通常
以"
更容易审查的方式推出 。
因此，如果我们回到咖啡馆的类比，
"这就像
少数私人演员"
拥有所有原料，
他们在全球范围内生产所有三明治，
并且没有太多监管。
"因此，此时
您可能会感到害怕，"
并且可能会感到有点不舒服。
"这很讽刺，因为几分钟前，
我打算给你们买三明治，"
而你答应了。
"这就是为什么你不应该接受
陌生人的食物。"
"但如果我不是同样乐观的话，我就不会站在这里。
"
"那是因为我认为
我们有"
"监管
和文化变革背后的动力。"
"特别是如果我们遵守有关
"
"企业
应如何处理数据的三个基本原则。"
"第一个原则是
收集数据的公司应该告诉我们"
他们正在收集什么。
"这将使我们能够提出
这样的问题：它是否受版权保护？"
该信息属于私人信息吗？
你能停下来吗？
"它还
向科学探究开放了数据。"
"第二个原则是，
收集我们数据的公司应该在对这些数据采取任何行动之前告诉我们"
"他们将用这些数据做什么
。"
"通过要求公司
告诉我们他们的计划，"
这意味着他们必须有一个计划，
这将是一个伟大的第一步。
"它还可能会导致
数据捕获的最小化，"
"因为
"
"如果他们不知道自己要如何处理数据，他们将无法捕获数据
。"
最后，原则三，
"构建人工智能的公司
应该告诉我们"
他们用于训练人工智能的数据。
"这就是
数据集营养标签"
"和其他透明度标签
发挥作用的地方。"
"您知道，在大多数情况下数据
本身无法提供的情况下"
，
"标签对于我们
能够调查成分"
并开始寻找解决方案至关重要。
所以我想告诉你们一个好消息，
"那就是数据
营养项目和其他项目"
只是全球人工智能问责运动的一小部分
。
"数据集营养标签
和其他项目只是第一步。"
监管即将出台，
文化规范正在发生变化，
"特别是如果我们
遵守这三个基本原则，"
"即公司应该告诉我们他们
正在收集什么，在他们做任何事情之前"
"告诉我们他们将用它做什么
，"
以及 构建人工智能的公司
"应该解释
他们用来构建系统的数据。"
"我们需要让
这些组织"
对他们正在构建的人工智能负责
"，就像我们
对食品行业所做的那样，询问他们"
里面有什么以及你是如何制作的？
"只有这样，我们才能在问题
发生之前（"
而不是发生之后）缓解问题。
"在此过程中，创建
一个"
对每个人都更健康的集成算法互联网。
谢谢。
（掌声）